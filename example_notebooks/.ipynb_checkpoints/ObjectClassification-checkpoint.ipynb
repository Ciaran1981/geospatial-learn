{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object based training and classification using machine learning\n",
    "----------------------\n",
    "\n",
    "### A simple land cover mapping example using aerial photography \n",
    "\n",
    "The following workflow demonstrates using image segmentation and machine learning to produce landcover classes, the priniciples of which broadly apply to much remote sensing work. Image segmentation  then classification of the resulting attributes is often used to reduce noisy/detailed imagery to lancover classes greater in spatial coverage than the detail within the image.   \n",
    "\n",
    "The data:\n",
    "\n",
    "- Colour infra-red imagery collected over ther Arun Valley, Sussex (G, R, NiR), courtesy of the now defunct LandMap website\n",
    "\n",
    "- shapefiles of the training samples\n",
    "\n",
    "**Disclaimer! I do not claim this is good model or set of classes!!** \n",
    "\n",
    "**This is merely to demonstrate the funtionality on a segmentation derived from fine spatial resolution data**\n",
    "\n",
    "Obviously, the reality of this type of approach is using large training sets and lengthier model parameter searches are required, as datasets are bigger than this small demonstration. \n",
    "\n",
    "**The Data**\n",
    "\n",
    "The data is available here\n",
    "\n",
    "https://drive.google.com/file/d/12GFv8473HdE1KoW0ESFXZgXCCGFysP0A/view?usp=sharing\n",
    "\n",
    "The zip contains:\n",
    "\n",
    "- Arundel_WWT.tif (a Green/Red/NIR composite)\n",
    "- ArunLcover.qml (a QGIS colour map for the results)\n",
    "- Arundel_seg_empty.shp (the result of segmentation performed in this analysis complete with training labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module imports\n",
    "----------------------\n",
    "\n",
    "As mentioned above, the code above imports the modules required for this image classification workflow, using the Object-based image analysis method on polygonal data.\n",
    "\n",
    "We use pyshp breifly to look at some attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geospatial_learn import learning, shape, raster\n",
    "import shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd 'my/path/LCover'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msRas = ('Arundel_WWT.tif')\n",
    "\n",
    "segShape = ('Arundel_seg_empty.shp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Imagery\n",
    "\n",
    "The training and imagery should look something like this by setting importing the symbology from ArunLcover.qml.\n",
    "\n",
    "Remeber to set the attribute to Train as this qml is set to RF, which is for the later results!\n",
    "\n",
    "\n",
    "<img src=\"figures/ArunTrain.png\" style=\"height:400px\">\n",
    "\n",
    "The classes used are:\n",
    "\n",
    "<img src=\"figures/ArunClasses.png\" style=\"height:100px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The segmentations that was used**\n",
    "\n",
    "**Included for reference only, I don't claim this algorithm/lib is the best solution, just an example**\n",
    "\n",
    "The example segmentation uses the OTB meanshift. \n",
    "\n",
    "The otb command for reference\n",
    "\n",
    "```bash\n",
    "otbcli_Segmentation -in path/to/image -filter meanshift -filter.meanshift.spatialr 5 -filter.meanshift.ranger 10 \n",
    "-filter.meanshift.minsize 50 -mode vector -mode.vector.out path/to/outShape\n",
    "```                     \n",
    "\n",
    "We prefix the command line with ! to execute in this notebook as this is external to python\n",
    "\n",
    "**Only run this if you wish to either experiment with parameters and/or wish to label the training segments yourself! Uncomment if this is the case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection of statistics\n",
    "----------------------------\n",
    "\n",
    "Segment attribute data is written to the labeled shapefile using the shape.zonal_stats and shape.texture_stats function from geospatial_learn.\n",
    "\n",
    "Rather than repeat the same line of code for each band, a simple for loop is used below to extract the band data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zonal stats\n",
    "# please note that by using enumerate we assume the bandnames are ordered as the are in the image!\n",
    "bandnames = ['g', 'r', 'nir']\n",
    "\n",
    "\n",
    "# Please note we add 1 to the bnd index as python counts from zero\n",
    "for bnd,name in enumerate(bandnames):\n",
    "    shape.zonal_stats(segShape, msRas, bnd+1, name+'mn', stat = 'mean', write_stat = True)\n",
    "    shape.zonal_stats(segShape, msRas, bnd+1, name+'mdn', stat = 'median', write_stat = True)\n",
    "    shape.zonal_stats(segShape, msRas, bnd+1, name+'skw', stat = 'skew', write_stat = True)\n",
    "    shape.zonal_stats(segShape, msRas, bnd+1, name+'krt', stat = 'kurtosis', write_stat = True)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texture props\n",
    "bandnames = ['g', 'r', 'nir']\n",
    "\n",
    "for bnd,name in enumerate(bandnames):\n",
    "    shape.texture_stats(segShape, msRas,  bnd+1,  gprop='entropy',  write_stat=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape props\n",
    "\n",
    "props = ['MajorAxisLength', 'MinorAxisLength', 'Area', 'Eccentricity', 'Solidity',\n",
    "         'Extent', 'Perimeter']\n",
    "for prop in props:\n",
    "    shape.shape_props(segShape, prop, label_field = 'DN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model training\n",
    "\n",
    "Now that we have collected our training statistics, we can calibrate our chosen model, which, in this case is a random forest.\n",
    "\n",
    "Geospatial_learn wraps scikit-learn (hence the name) and xgboost, two excellent machine learning libraries (I intend to add tensor flow to this shortly also!).\n",
    "\n",
    "Typically, we collect training from a shapefile and save it as a .gz file with the get_training_shp function, though it is possible to feed the training array straight into the create_model function.\n",
    "\n",
    "The label and feature fields respectively. \n",
    "\n",
    "```python\n",
    "\n",
    "   label_field = 'Train'\n",
    "\n",
    "   feat_fields = ['gmn','gmdn','gskw','gkrt','rmn','rmdn','rskw','rkrt','nirmn','nirmdn',\n",
    "                  'nirskw','nirkrt','entropy','entropy_1','entropy_2']\n",
    "```\n",
    "\n",
    "Path to the training array we intend to save. This is optional, it is possible to run\n",
    "\n",
    "```python\n",
    "\n",
    "   training = 'savemytraining.gz'\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "   dfTrain, rejects = learning.get_training_shp(segShape, label_field, feat_fields,  outFile = training)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivational lazyness\n",
    "\n",
    "**Here is a quick way of getting a list of the fields to be used as features in the training and classification without typing them out**\n",
    "\n",
    "Here a list comprehension is used with the pyshp (shapefile) lib. This is not really that important, but if you are interested, it is a useful python concept....\n",
    "\n",
    "If you are not familiar with list comprehensions, here is an attempt at explanation in the context of the shapefile. List comprehensions are useful, comapct and readable way of performing an operation on data.\n",
    "\n",
    "The shapefile is read in as a python class object. A class is a fundamental part of object-based programming  (nothing to with the current task!!!) where the data structure has: \n",
    "\n",
    "* properties - effectively attributes of the data\n",
    "\n",
    "* methods - built in operations on the data\n",
    "\n",
    "Below r is the 'object' read in by the pyshp library\n",
    "\n",
    "```python\n",
    "\n",
    "r = shapefile.Reader(segShape)\n",
    "```\n",
    "The shapefile has a 'fields' property, but this returns a list in which each entry has the field name, datatype length, value. So each entry contains superfluous info:\n",
    "```python\n",
    "\n",
    " [\"AREA\", \"N\", 18, 5],\n",
    "\n",
    "```\n",
    "\n",
    "Where as we are only interested in the first part of every entry, which is the field name. \n",
    "\n",
    "so this code is basically saying, output the first part (f[0]) of each entry (f) in the fields property (r.fields)\n",
    "\n",
    "```python\n",
    "[f[0] for f in r.fields]\n",
    "```\n",
    "Hence we end up with a list of only the field titles. Which gives us the names of all the properties.\n",
    "\n",
    "Finally we exclude the DN and Train fields by indexing.\n",
    "\n",
    "```python\n",
    "feat_fields[3:25]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = shapefile.Reader(segShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_fields = [f[0] for f in r.fields]\n",
    "feat_fields = feat_fields[3:25]\n",
    "feat_fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feat_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to create some training samples\n",
    "\n",
    "These can be used in both a pixel and object based manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables returned are a dataframe (like in R) and a list of reject polygons which may have had invalid geometry.\n",
    "\n",
    "In practice there shouldn't be many of these - it's just to make sure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = 'Arundel_train_ob.gz'\n",
    "\n",
    "label_field = 'Train'\n",
    "\n",
    "dfTrain, rejects = learning.get_training_shp(segShape, label_field, feat_fields,  outFile = training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick look at the returned dataframe to see the information. Dataframes are handy to visualise in jupyter and ipython. The create_model accepts the raw array data though so the .as_matrix() property of the dataframe is used when inputting to create_model\n",
    "\n",
    "The first column is the label, the rest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation with k-fold cross validated grid search\n",
    "\n",
    "Now we can train the model with the above data.\n",
    "\n",
    "We first define the parameters we wish to grid search over. The parameters below are just an example, It is of course possible for these to be more numerous at the cost of processing time. The time is a function of the number of possibilities per parameter.\n",
    "\n",
    "```python\n",
    "\n",
    "    params = {'n_estimators': [500], 'max_features': ['sqrt', 'log2'], \n",
    "              'min_samples_split':[5,10,20,50], 'min_samples_leaf': [5,10,20,50]}\n",
    "```          \n",
    "When we execute the create_model function we get a summary of the no of model fits\n",
    "\n",
    "'Fitting 5 folds for each of 18 candidates, totalling 90 fits'\n",
    "\n",
    "I have fixed the n_estimators (trees) at 500 below but this could be varied also.\n",
    "\n",
    "For a full list of params see:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': [500], 'max_features': ['sqrt', 'log2'], \n",
    "          'min_samples_split':[5,10,20,50], 'min_samples_leaf': [5,10,20,50]}\n",
    "\n",
    "outModel = 'Arundel_Rf_model2.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_model function is executed below. The progress of the gird search is printed out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.create_model(dfTrain.as_matrix(), outModel, clf='rf', cv=5, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it is worth checking the shapefile to see everything is written correctly in qgis....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classification \n",
    "\n",
    "Having collected training and created a model, we can finally classify our polygon attributes.\n",
    "\n",
    "This is done using the classify object function where:\n",
    "\n",
    "```python\n",
    "\n",
    "   learning.classify_object(outModel, inShape, feat_fields, field_name='RF')\n",
    "   \n",
    "```\n",
    "\n",
    "We reuse the outModel, inShape and feat_fields variables from earlier which leaves only the keyword arg field_name, which is what we intend to call the field holding the classification values.\n",
    "\n",
    "Keep this short if writing to ESRI shapefiles as there is a strict limit to the no of characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.classify_object(outModel, segShape, feat_fields, field_name='RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model evaluation\n",
    "\n",
    "Have a look at the results in QGIS by using the style file provided\n",
    "\n",
    "<img src=\"figures/ArunRF.png\" style=\"height:400px\">\n",
    "\n",
    "<img src=\"figures/ArunClasses.png\" style=\"height:100px\">\n",
    "\n",
    "\n",
    "Having classified the segmentation - it looks fairly convincing, though some things appear strange, such as slivers of area in the fields classified as trees - could be explored with feature importance. \n",
    "\n",
    "The classifier may be splitting nodes on the basis of a geometric feature which is not actually indicative of trees!\n",
    "\n",
    "Additionally part of the ponds on the top right are mistaken as impervious, though  superficially they are a similar colour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.plot_feature_importances(outModel, feat_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though kurtosis is not contributing much to the results, neither many spatial properties. MjAxis is however, and may be the reason for the spurious tree mapping. Removing it **could** eliminate this issue.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
